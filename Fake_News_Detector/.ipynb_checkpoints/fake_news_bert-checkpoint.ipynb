{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter \n",
    "import http.client\n",
    "import json\n",
    "\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(link_conn) :\n",
    "    try :\n",
    "        conn = http.client.HTTPSConnection(link_conn)\n",
    "    except :\n",
    "        conn = http.client.HTTPSConnection('localhost:5000')\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    conn.request('GET', '/getAllUntestedPosts', None, headers)\n",
    "    response = conn.getresponse()\n",
    "    return response.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(batch_size, epoch_size, filename) :\n",
    "    '''very unethical way of loading and traing the data in same function'''\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    train_data, test_data = imdb_dataset(train=True, test=True)\n",
    "    df = pd.read_csv(\"./data/fake.csv\")\n",
    "    df = df[['text', 'type']]\n",
    "    #print(len(df))\n",
    "\n",
    "    #print(Counter(df['type'].values))\n",
    "\n",
    "    df = df[df['type'].isin(['fake', 'satire'])]\n",
    "    df.dropna(inplace = True)\n",
    "    df_fake = df[df['type'] == 'fake'] \n",
    "    df_statire = df[df['type'] == 'satire'] \n",
    "    df_statire = df_statire.sample(n=len(df_fake))\n",
    "    df = df_statire.append(df_fake)\n",
    "    df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
    "\n",
    "    #print(Counter(df['type'].values))\n",
    "\n",
    "    train_data = df.head(19)\n",
    "    test_data = df.tail(19)\n",
    "\n",
    "    #print(train_data)\n",
    "    train_data = [{'text': text, 'type': type_data } for text in list(train_data['text']) for type_data in list(train_data['type'])]\n",
    "    test_data = [{'text': text, 'type': type_data } for text in list(test_data['text']) for type_data in list(test_data['type'])]\n",
    "\n",
    "    train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
    "    test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))\n",
    "\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
    "    test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
    "\n",
    "    train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "    test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "    test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "\n",
    "    train_y = np.array(train_labels) == 'fake'\n",
    "    test_y = np.array(test_labels) == 'fake'\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    EPOCHS = epoch_size\n",
    "\n",
    "\n",
    "    train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "    test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "    train_masks_tensor = torch.tensor(train_masks)\n",
    "    test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "    train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "    train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "    test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "    test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "    train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "    train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "    test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "    test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "    test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    bert_clf = BertBinaryClassifier()\n",
    "    optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "    \n",
    "    for epoch_num in range(EPOCHS):\n",
    "        bert_clf.train()\n",
    "        train_loss = 0\n",
    "        for step_num, batch_data in enumerate(train_dataloader):\n",
    "            token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "            probas = bert_clf(token_ids, masks)\n",
    "            loss_func = nn.BCELoss()\n",
    "            batch_loss = loss_func(probas, labels)\n",
    "            train_loss += batch_loss.item()\n",
    "            bert_clf.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Epoch: ', epoch_num + 1)\n",
    "            print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "\n",
    "    torch.save(bert_clf, filename)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(df, filename) :\n",
    "    \n",
    "    #input\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    l = len(df)\n",
    "    lst = []\n",
    "    if(l%2 == 1):\n",
    "        for i in range(l) :\n",
    "            if(i%2 == 1) :\n",
    "                lst.append('fake')\n",
    "            else:\n",
    "                lst.append('satire')\n",
    "    else :\n",
    "        for i in range(l) :\n",
    "            if(i%2 == 0) :\n",
    "                lst.append('fake')\n",
    "            else:\n",
    "                lst.append('satire')\n",
    "    df['type'] = lst\n",
    "    test_data = df\n",
    "    test_data1 = [{'text': text, 'type': \"\"} for text in list(test_data['text']) ]\n",
    "\n",
    "    i = 0\n",
    "    for type_data in list(test_data['type']) :\n",
    "        test_data1[i]['type'] = type_data\n",
    "        i = i + 1\n",
    "        \n",
    "    test_data = test_data1\n",
    "    test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
    "    test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "    test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "    test_y = np.array(test_labels) == 'fake'\n",
    "    test_y = torch.from_numpy(np.array(test_y, dtype=np.uint8))\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    \n",
    "    test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "    test_masks_tensor = torch.tensor(test_masks)\n",
    "    test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "    test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "    test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "    test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "    test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    bert_clf = BertBinaryClassifier()\n",
    "    optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "    bert_clf = torch.load(filename)\n",
    "    bert_clf.eval()\n",
    "    bert_predicted = []\n",
    "    all_logits = []\n",
    "    lst = []\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "            token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "\n",
    "            logits = bert_clf(token_ids, masks)\n",
    "            lst.append(logits)\n",
    "            loss_func = nn.BCELoss()\n",
    "            loss = loss_func(logits, labels)\n",
    "            numpy_logits = logits.cpu().detach().numpy()\n",
    "\n",
    "            bert_predicted += list(numpy_logits[:, 0])\n",
    "            all_logits += list(numpy_logits[:, 0])\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_post(article, score, link_conn) :\n",
    "    try :\n",
    "        conn = http.client.HTTPSConnection(link_conn)\n",
    "    except :\n",
    "        conn = http.client.HTTPSConnection('localhost:5000')\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = article\n",
    "    data['fakeness'] = str(score)\n",
    "    json_data = json.dumps(data)\n",
    "    conn.request('POST', '/fakePost', json_data, headers)\n",
    "    response = conn.getresponse()\n",
    "    print(response.read().decode())\n",
    "    return\n",
    "  \n",
    "def true_post(article, score, link_conn) :\n",
    "    try :\n",
    "        conn = http.client.HTTPSConnection(link_conn)\n",
    "    except :\n",
    "        conn = http.client.HTTPSConnection('localhost:5000')\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = article\n",
    "    data['fakeness'] = str(score)\n",
    "    json_data = json.dumps(data)\n",
    "    conn.request('POST', '/testedPost', json_data, headers)\n",
    "    response = conn.getresponse()\n",
    "    print(response.read().decode())\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_bert(lst_bert, filename, link_conn) :\n",
    "    lst = []\n",
    "    for t in lst_bert :\n",
    "        lst.append(t['content'])\n",
    "    df = pd.DataFrame(lst, columns=[\"text\"])\n",
    "    scores = testing(df, filename)\n",
    "    #print(scores)\n",
    "    i = 0\n",
    "    for article in lst_bert :\n",
    "        if(score[i] > 0.5 ) :\n",
    "            fake_post(article, score[i], link_conn)\n",
    "        else :\n",
    "            true_post(article, score[i], link_conn)\n",
    "        i = i + 1\n",
    "    return\n",
    "            \n",
    "def malicious_url_detector(lst_url, link_conn) :\n",
    "    lst = []\n",
    "    for t in lst_url :\n",
    "        lst.append(t['url'])\n",
    "    #df = pd.DataFrame(lst, columns=[\"url\"])\n",
    "    data = pd.read_csv(\"./data/url.csv\")\n",
    "    y = data[\"label\"]\n",
    "    url_list = data[\"url\"]\n",
    "    # Using Tokenizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Store vectors into X variable as Our XFeatures\n",
    "    X = vectorizer.fit_transform(url_list)\n",
    "    # Split into training and testing dataset 80:20 ratio\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Model Building using logistic regression\n",
    "    logit = LogisticRegression()\n",
    "    logit.fit(X, y)\n",
    "    \n",
    "    test_data = vectorizer.fit_transform(lst)\n",
    "    scores = logit.predict(test_data)\n",
    "    i = 0\n",
    "    for article in lst_bert :\n",
    "        if(score[i] > 0.5 ) :\n",
    "            fake_post(article, score[i], link_conn)\n",
    "        else :\n",
    "            true_post(article, score[i], link_conn)\n",
    "        i = i + 1\n",
    "    return\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_conn = \"\"    #put ngrok-link here\n",
    "data = get_data(link_conn) \n",
    "data = json.loads(data)\n",
    "lst_bert = []\n",
    "lst_url = []\n",
    "for article in data:\n",
    "    try:\n",
    "        if(len(article['url'] != 0) :\n",
    "           lst_url.append(article)\n",
    "        else :\n",
    "           lst_bert.append(article)\n",
    "   except:\n",
    "           lst_bert.append(article)\n",
    "if(len(lst_url) != 0) :\n",
    "    malicious_url_detector(lst_url, link_conn)\n",
    "if(len(lst_bert) != 0):\n",
    "    using_bert(lst_bert, \"./checkpoints/fake_news_bert.pt\", link_conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
